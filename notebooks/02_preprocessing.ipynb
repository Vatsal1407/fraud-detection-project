{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251c6e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# For handling imbalanced data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac6b61b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 200,000 rows, 24 columns\n",
      "Fraud cases: 10,088 (5.04%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_ID</th>\n",
       "      <th>Customer_Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>Bank_Branch</th>\n",
       "      <th>Account_Type</th>\n",
       "      <th>Transaction_ID</th>\n",
       "      <th>Transaction_Date</th>\n",
       "      <th>Transaction_Time</th>\n",
       "      <th>Transaction_Amount</th>\n",
       "      <th>Merchant_ID</th>\n",
       "      <th>Transaction_Type</th>\n",
       "      <th>Merchant_Category</th>\n",
       "      <th>Account_Balance</th>\n",
       "      <th>Transaction_Device</th>\n",
       "      <th>Transaction_Location</th>\n",
       "      <th>Device_Type</th>\n",
       "      <th>Is_Fraud</th>\n",
       "      <th>Transaction_Currency</th>\n",
       "      <th>Customer_Contact</th>\n",
       "      <th>Transaction_Description</th>\n",
       "      <th>Customer_Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d5f6ec07-d69e-4f47-b9b4-7c58ff17c19e</td>\n",
       "      <td>Osha Tella</td>\n",
       "      <td>Male</td>\n",
       "      <td>60</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>Thiruvananthapuram</td>\n",
       "      <td>Thiruvananthapuram Branch</td>\n",
       "      <td>Savings</td>\n",
       "      <td>4fa3208f-9e23-42dc-b330-844829d0c12c</td>\n",
       "      <td>23-01-2025</td>\n",
       "      <td>16:04:07</td>\n",
       "      <td>32415.45</td>\n",
       "      <td>214e03c5-5c34-40d1-a66c-f440aa2bbd02</td>\n",
       "      <td>Transfer</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>74557.27</td>\n",
       "      <td>Voice Assistant</td>\n",
       "      <td>Thiruvananthapuram, Kerala</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>INR</td>\n",
       "      <td>+9198579XXXXXX</td>\n",
       "      <td>Bitcoin transaction</td>\n",
       "      <td>oshaXXXXX@XXXXX.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7c14ad51-781a-4db9-b7bd-67439c175262</td>\n",
       "      <td>Hredhaan Khosla</td>\n",
       "      <td>Female</td>\n",
       "      <td>51</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>Nashik</td>\n",
       "      <td>Nashik Branch</td>\n",
       "      <td>Business</td>\n",
       "      <td>c9de0c06-2c4c-40a9-97ed-3c7b8f97c79c</td>\n",
       "      <td>11-01-2025</td>\n",
       "      <td>17:14:53</td>\n",
       "      <td>43622.60</td>\n",
       "      <td>f9e3f11f-28d3-4199-b0ca-f225a155ede6</td>\n",
       "      <td>Bill Payment</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>74622.66</td>\n",
       "      <td>POS Mobile Device</td>\n",
       "      <td>Nashik, Maharashtra</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>0</td>\n",
       "      <td>INR</td>\n",
       "      <td>+9191074XXXXXX</td>\n",
       "      <td>Grocery delivery</td>\n",
       "      <td>hredhaanXXXX@XXXXXX.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3a73a0e5-d4da-45aa-85f3-528413900a35</td>\n",
       "      <td>Ekani Nazareth</td>\n",
       "      <td>Male</td>\n",
       "      <td>20</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>Bhagalpur</td>\n",
       "      <td>Bhagalpur Branch</td>\n",
       "      <td>Savings</td>\n",
       "      <td>e41c55f9-c016-4ff3-872b-cae72467c75c</td>\n",
       "      <td>25-01-2025</td>\n",
       "      <td>03:09:52</td>\n",
       "      <td>63062.56</td>\n",
       "      <td>97977d83-5486-4510-af1c-8dada3e1cfa0</td>\n",
       "      <td>Bill Payment</td>\n",
       "      <td>Groceries</td>\n",
       "      <td>66817.99</td>\n",
       "      <td>ATM</td>\n",
       "      <td>Bhagalpur, Bihar</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>0</td>\n",
       "      <td>INR</td>\n",
       "      <td>+9197745XXXXXX</td>\n",
       "      <td>Mutual fund investment</td>\n",
       "      <td>ekaniXXX@XXXXXX.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7902f4ef-9050-4a79-857d-9c2ea3181940</td>\n",
       "      <td>Yamini Ramachandran</td>\n",
       "      <td>Female</td>\n",
       "      <td>57</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Chennai Branch</td>\n",
       "      <td>Business</td>\n",
       "      <td>7f7ee11b-ff2c-45a3-802a-49bc47c02ecb</td>\n",
       "      <td>19-01-2025</td>\n",
       "      <td>12:27:02</td>\n",
       "      <td>14000.72</td>\n",
       "      <td>f45cd6b3-5092-44d0-8afb-490894605184</td>\n",
       "      <td>Debit</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>58177.08</td>\n",
       "      <td>POS Mobile App</td>\n",
       "      <td>Chennai, Tamil Nadu</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>0</td>\n",
       "      <td>INR</td>\n",
       "      <td>+9195889XXXXXX</td>\n",
       "      <td>Food delivery</td>\n",
       "      <td>yaminiXXXXX@XXXXXXX.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3a4bba70-d9a9-4c5f-8b92-1735fd8c19e9</td>\n",
       "      <td>Kritika Rege</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>Amritsar</td>\n",
       "      <td>Amritsar Branch</td>\n",
       "      <td>Savings</td>\n",
       "      <td>f8e6ac6f-81a1-4985-bf12-f60967d852ef</td>\n",
       "      <td>30-01-2025</td>\n",
       "      <td>18:30:46</td>\n",
       "      <td>18335.16</td>\n",
       "      <td>70dd77dd-3b00-4b2c-8ebc-cfb8af5f6741</td>\n",
       "      <td>Transfer</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>16108.56</td>\n",
       "      <td>Virtual Card</td>\n",
       "      <td>Amritsar, Punjab</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>0</td>\n",
       "      <td>INR</td>\n",
       "      <td>+9195316XXXXXX</td>\n",
       "      <td>Debt repayment</td>\n",
       "      <td>kritikaXXXX@XXXXXX.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Customer_ID        Customer_Name  Gender  Age  \\\n",
       "0  d5f6ec07-d69e-4f47-b9b4-7c58ff17c19e           Osha Tella    Male   60   \n",
       "1  7c14ad51-781a-4db9-b7bd-67439c175262      Hredhaan Khosla  Female   51   \n",
       "2  3a73a0e5-d4da-45aa-85f3-528413900a35       Ekani Nazareth    Male   20   \n",
       "3  7902f4ef-9050-4a79-857d-9c2ea3181940  Yamini Ramachandran  Female   57   \n",
       "4  3a4bba70-d9a9-4c5f-8b92-1735fd8c19e9         Kritika Rege  Female   43   \n",
       "\n",
       "         State                City                Bank_Branch Account_Type  \\\n",
       "0       Kerala  Thiruvananthapuram  Thiruvananthapuram Branch      Savings   \n",
       "1  Maharashtra              Nashik              Nashik Branch     Business   \n",
       "2        Bihar           Bhagalpur           Bhagalpur Branch      Savings   \n",
       "3   Tamil Nadu             Chennai             Chennai Branch     Business   \n",
       "4       Punjab            Amritsar            Amritsar Branch      Savings   \n",
       "\n",
       "                         Transaction_ID Transaction_Date Transaction_Time  \\\n",
       "0  4fa3208f-9e23-42dc-b330-844829d0c12c       23-01-2025         16:04:07   \n",
       "1  c9de0c06-2c4c-40a9-97ed-3c7b8f97c79c       11-01-2025         17:14:53   \n",
       "2  e41c55f9-c016-4ff3-872b-cae72467c75c       25-01-2025         03:09:52   \n",
       "3  7f7ee11b-ff2c-45a3-802a-49bc47c02ecb       19-01-2025         12:27:02   \n",
       "4  f8e6ac6f-81a1-4985-bf12-f60967d852ef       30-01-2025         18:30:46   \n",
       "\n",
       "   Transaction_Amount                           Merchant_ID Transaction_Type  \\\n",
       "0            32415.45  214e03c5-5c34-40d1-a66c-f440aa2bbd02         Transfer   \n",
       "1            43622.60  f9e3f11f-28d3-4199-b0ca-f225a155ede6     Bill Payment   \n",
       "2            63062.56  97977d83-5486-4510-af1c-8dada3e1cfa0     Bill Payment   \n",
       "3            14000.72  f45cd6b3-5092-44d0-8afb-490894605184            Debit   \n",
       "4            18335.16  70dd77dd-3b00-4b2c-8ebc-cfb8af5f6741         Transfer   \n",
       "\n",
       "  Merchant_Category  Account_Balance Transaction_Device  \\\n",
       "0        Restaurant         74557.27    Voice Assistant   \n",
       "1        Restaurant         74622.66  POS Mobile Device   \n",
       "2         Groceries         66817.99                ATM   \n",
       "3     Entertainment         58177.08     POS Mobile App   \n",
       "4     Entertainment         16108.56       Virtual Card   \n",
       "\n",
       "         Transaction_Location Device_Type  Is_Fraud Transaction_Currency  \\\n",
       "0  Thiruvananthapuram, Kerala         POS         0                  INR   \n",
       "1         Nashik, Maharashtra     Desktop         0                  INR   \n",
       "2            Bhagalpur, Bihar     Desktop         0                  INR   \n",
       "3         Chennai, Tamil Nadu      Mobile         0                  INR   \n",
       "4            Amritsar, Punjab      Mobile         0                  INR   \n",
       "\n",
       "  Customer_Contact Transaction_Description           Customer_Email  \n",
       "0   +9198579XXXXXX     Bitcoin transaction      oshaXXXXX@XXXXX.com  \n",
       "1   +9191074XXXXXX        Grocery delivery  hredhaanXXXX@XXXXXX.com  \n",
       "2   +9197745XXXXXX  Mutual fund investment      ekaniXXX@XXXXXX.com  \n",
       "3   +9195889XXXXXX           Food delivery  yaminiXXXXX@XXXXXXX.com  \n",
       "4   +9195316XXXXXX          Debt repayment   kritikaXXXX@XXXXXX.com  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('../data/Bank_Transaction_Fraud_Detection.csv')\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "print(f\"Fraud cases: {df['Is_Fraud'].sum():,} ({df['Is_Fraud'].mean()*100:.2f}%)\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59002b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING: TIME-BASED FEATURES\n",
      "================================================================================\n",
      "Time-based features created:\n",
      "  â€¢ Hour (0-23)\n",
      "  â€¢ Day_of_Week (0=Mon, 6=Sun)\n",
      "  â€¢ Day, Month, Year\n",
      "  â€¢ Is_Weekend (0/1)\n",
      "  â€¢ Is_Night (0/1)\n",
      "  â€¢ Is_Business_Hours (0/1)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING: TIME-BASED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Parse datetime\n",
    "df['Transaction_DateTime'] = pd.to_datetime(df['Transaction_Date'] + ' ' + df['Transaction_Time'], \n",
    "                                             format='%d-%m-%Y %H:%M:%S')\n",
    "\n",
    "# Extract time components\n",
    "df['Hour'] = df['Transaction_DateTime'].dt.hour\n",
    "df['Day_of_Week'] = df['Transaction_DateTime'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "df['Day'] = df['Transaction_DateTime'].dt.day\n",
    "df['Month'] = df['Transaction_DateTime'].dt.month\n",
    "df['Year'] = df['Transaction_DateTime'].dt.year\n",
    "\n",
    "# Create time-based categories\n",
    "df['Is_Weekend'] = df['Day_of_Week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "df['Is_Night'] = df['Hour'].apply(lambda x: 1 if x >= 22 or x <= 6 else 0)\n",
    "df['Is_Business_Hours'] = df['Hour'].apply(lambda x: 1 if 9 <= x <= 17 else 0)\n",
    "\n",
    "print(\"Time-based features created:\")\n",
    "print(\"  â€¢ Hour (0-23)\")\n",
    "print(\"  â€¢ Day_of_Week (0=Mon, 6=Sun)\")\n",
    "print(\"  â€¢ Day, Month, Year\")\n",
    "print(\"  â€¢ Is_Weekend (0/1)\")\n",
    "print(\"  â€¢ Is_Night (0/1)\")\n",
    "print(\"  â€¢ Is_Business_Hours (0/1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9531764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING: TRANSACTION-BASED FEATURES\n",
      "================================================================================\n",
      "Transaction features created:\n",
      "  â€¢ Transaction_to_Balance_Ratio\n",
      "  â€¢ Is_High_Value (>74315)\n",
      "  â€¢ Is_Low_Balance (<28742)\n",
      "  â€¢ Amount_Category (Low/Medium/High/Very High)\n",
      "\n",
      "Sample of new features:\n",
      "   Transaction_Amount  Account_Balance  Transaction_to_Balance_Ratio  \\\n",
      "0            32415.45         74557.27                      0.434772   \n",
      "1            43622.60         74622.66                      0.584576   \n",
      "2            63062.56         66817.99                      0.943796   \n",
      "3            14000.72         58177.08                      0.240657   \n",
      "4            18335.16         16108.56                      1.138225   \n",
      "5             9711.15         61258.85                      0.158526   \n",
      "6            94677.01         36313.61                      2.607205   \n",
      "7            67704.28         16948.73                      3.994652   \n",
      "8            72953.45         18138.71                      4.021976   \n",
      "9             5689.02         65801.35                      0.086457   \n",
      "\n",
      "   Is_High_Value  Is_Low_Balance Amount_Category  \n",
      "0              0               0          Medium  \n",
      "1              0               0          Medium  \n",
      "2              0               0            High  \n",
      "3              0               0             Low  \n",
      "4              0               1             Low  \n",
      "5              0               0             Low  \n",
      "6              1               0       Very High  \n",
      "7              0               1            High  \n",
      "8              0               1            High  \n",
      "9              0               0             Low  \n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING: TRANSACTION-BASED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Transaction to Balance Ratio\n",
    "df['Transaction_to_Balance_Ratio'] = df['Transaction_Amount'] / df['Account_Balance']\n",
    "\n",
    "# High value transaction flag (above 75th percentile)\n",
    "transaction_75th = df['Transaction_Amount'].quantile(0.75)\n",
    "df['Is_High_Value'] = (df['Transaction_Amount'] > transaction_75th).astype(int)\n",
    "\n",
    "# Low balance flag (below 25th percentile)\n",
    "balance_25th = df['Account_Balance'].quantile(0.25)\n",
    "df['Is_Low_Balance'] = (df['Account_Balance'] < balance_25th).astype(int)\n",
    "\n",
    "# Transaction amount bins\n",
    "df['Amount_Category'] = pd.cut(df['Transaction_Amount'], \n",
    "                                bins=[0, 25000, 50000, 75000, 100000],\n",
    "                                labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "print(\"Transaction features created:\")\n",
    "print(f\"  â€¢ Transaction_to_Balance_Ratio\")\n",
    "print(f\"  â€¢ Is_High_Value (>{transaction_75th:.0f})\")\n",
    "print(f\"  â€¢ Is_Low_Balance (<{balance_25th:.0f})\")\n",
    "print(f\"  â€¢ Amount_Category (Low/Medium/High/Very High)\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of new features:\")\n",
    "print(df[['Transaction_Amount', 'Account_Balance', 'Transaction_to_Balance_Ratio', \n",
    "          'Is_High_Value', 'Is_Low_Balance', 'Amount_Category']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f1c2865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE SELECTION\n",
      "================================================================================\n",
      "Features to DROP: 12\n",
      "Categorical features: 10\n",
      "Numerical features: 13\n",
      "Target: Is_Fraud\n",
      "\n",
      "Total features for modeling: 23\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Features to DROP (not useful for modeling)\n",
    "drop_features = [\n",
    "    'Customer_ID',           # Unique identifier\n",
    "    'Customer_Name',         # Personal info\n",
    "    'Transaction_ID',        # Unique identifier\n",
    "    'Merchant_ID',           # Too many unique values\n",
    "    'Transaction_Date',      # Already extracted features\n",
    "    'Transaction_Time',      # Already extracted features\n",
    "    'Transaction_DateTime',  # Already extracted features\n",
    "    'Transaction_Location',  # Too many unique values (can use State/City instead)\n",
    "    'Customer_Contact',      # Personal info\n",
    "    'Customer_Email',        # Personal info\n",
    "    'Transaction_Currency',  # All same (INR)\n",
    "    'Bank_Branch',          # High cardinality, use State/City instead\n",
    "]\n",
    "\n",
    "# Categorical features (need encoding)\n",
    "categorical_features = [\n",
    "    'Gender',\n",
    "    'State',\n",
    "    'City', \n",
    "    'Account_Type',\n",
    "    'Transaction_Type',\n",
    "    'Merchant_Category',\n",
    "    'Transaction_Device',\n",
    "    'Device_Type',\n",
    "    'Transaction_Description',\n",
    "    'Amount_Category'\n",
    "]\n",
    "\n",
    "# Numerical features\n",
    "numerical_features = [\n",
    "    'Age',\n",
    "    'Transaction_Amount',\n",
    "    'Account_Balance',\n",
    "    'Hour',\n",
    "    'Day_of_Week',\n",
    "    'Day',\n",
    "    'Month',\n",
    "    'Transaction_to_Balance_Ratio',\n",
    "    'Is_Weekend',\n",
    "    'Is_Night',\n",
    "    'Is_Business_Hours',\n",
    "    'Is_High_Value',\n",
    "    'Is_Low_Balance'\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target = 'Is_Fraud'\n",
    "\n",
    "print(f\"Features to DROP: {len(drop_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"\\nTotal features for modeling: {len(categorical_features) + len(numerical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af37d957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HANDLING HIGH CARDINALITY FEATURES\n",
      "================================================================================\n",
      "Unique values in categorical features:\n",
      "  â€¢ Gender: 2 unique values\n",
      "  â€¢ State: 34 unique values\n",
      "  â€¢ City: 145 unique values\n",
      "  â€¢ Account_Type: 3 unique values\n",
      "  â€¢ Transaction_Type: 5 unique values\n",
      "  â€¢ Merchant_Category: 6 unique values\n",
      "  â€¢ Transaction_Device: 20 unique values\n",
      "  â€¢ Device_Type: 4 unique values\n",
      "  â€¢ Transaction_Description: 172 unique values\n",
      "  â€¢ Amount_Category: 4 unique values\n",
      "\n",
      "State - Top 5 most frequent:\n",
      "State\n",
      "Nagaland         6031\n",
      "Meghalaya        6003\n",
      "Uttar Pradesh    6002\n",
      "Uttarakhand      5985\n",
      "Lakshadweep      5954\n",
      "Name: count, dtype: int64\n",
      "\n",
      "City - Top 5 most frequent:\n",
      "City\n",
      "Chandigarh     8135\n",
      "Kavaratti      5954\n",
      "Udaipur        2681\n",
      "Daman          2022\n",
      "Car Nicobar    1956\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Transaction_Description - Top 5 most frequent:\n",
      "Transaction_Description\n",
      "Sports ticket            1268\n",
      "Home appliance repair    1257\n",
      "Taxi fare                1248\n",
      "Seminar registration     1246\n",
      "Taxi booking             1240\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ“ High cardinality features converted to frequency encoding\n",
      "Updated categorical features: 7\n",
      "Updated numerical features: 16\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"HANDLING HIGH CARDINALITY FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check cardinality of categorical features\n",
    "print(\"Unique values in categorical features:\")\n",
    "for col in categorical_features:\n",
    "    print(f\"  â€¢ {col}: {df[col].nunique()} unique values\")\n",
    "\n",
    "# For State and City - too many unique values, let's use frequency encoding\n",
    "# This converts categories to their frequency (how often they appear)\n",
    "\n",
    "def frequency_encoding(df, column):\n",
    "    \"\"\"Convert categorical column to frequency encoding\"\"\"\n",
    "    freq_encoding = df[column].value_counts(normalize=True).to_dict()\n",
    "    return df[column].map(freq_encoding)\n",
    "\n",
    "# Apply frequency encoding to high cardinality features\n",
    "high_cardinality_features = ['State', 'City', 'Transaction_Description']\n",
    "\n",
    "for col in high_cardinality_features:\n",
    "    df[f'{col}_Frequency'] = frequency_encoding(df, col)\n",
    "    print(f\"\\n{col} - Top 5 most frequent:\")\n",
    "    print(df[col].value_counts().head())\n",
    "\n",
    "# Remove original high cardinality features from categorical list\n",
    "categorical_features = [f for f in categorical_features if f not in high_cardinality_features]\n",
    "\n",
    "# Add frequency encoded features to numerical list\n",
    "numerical_features.extend([f'{col}_Frequency' for col in high_cardinality_features])\n",
    "\n",
    "print(f\"\\nâœ“ High cardinality features converted to frequency encoding\")\n",
    "print(f\"Updated categorical features: {len(categorical_features)}\")\n",
    "print(f\"Updated numerical features: {len(numerical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e56062c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENCODING CATEGORICAL VARIABLES\n",
      "================================================================================\n",
      "\n",
      "Categorical features to encode: ['Gender', 'Account_Type', 'Transaction_Type', 'Merchant_Category', 'Transaction_Device', 'Device_Type', 'Amount_Category']\n",
      "Number of features: 7\n",
      "\n",
      "âœ“ One-Hot Encoding applied\n",
      "Shape before encoding: (200000, 40)\n",
      "Shape after encoding: (200000, 70)\n",
      "New features created: 30\n",
      "\n",
      "Sample of new encoded columns (first 5 rows, showing some encoded features):\n",
      "   Gender_Male  Account_Type_Checking  Account_Type_Savings  \\\n",
      "0            1                      0                     1   \n",
      "1            0                      0                     0   \n",
      "2            1                      0                     1   \n",
      "3            0                      0                     0   \n",
      "4            0                      0                     1   \n",
      "\n",
      "   Transaction_Type_Credit  Transaction_Type_Debit  Transaction_Type_Transfer  \\\n",
      "0                        0                       0                          1   \n",
      "1                        0                       0                          0   \n",
      "2                        0                       0                          0   \n",
      "3                        0                       1                          0   \n",
      "4                        0                       0                          1   \n",
      "\n",
      "   Transaction_Type_Withdrawal  Merchant_Category_Electronics  \\\n",
      "0                            0                              0   \n",
      "1                            0                              0   \n",
      "2                            0                              0   \n",
      "3                            0                              0   \n",
      "4                            0                              0   \n",
      "\n",
      "   Merchant_Category_Entertainment  Merchant_Category_Groceries  \n",
      "0                                0                            0  \n",
      "1                                0                            0  \n",
      "2                                0                            1  \n",
      "3                                1                            0  \n",
      "4                                1                            0  \n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ENCODING CATEGORICAL VARIABLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# One-Hot Encoding for categorical features\n",
    "print(f\"\\nCategorical features to encode: {categorical_features}\")\n",
    "print(f\"Number of features: {len(categorical_features)}\")\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "df_encoded = pd.get_dummies(df_processed, columns=categorical_features, drop_first=True, dtype=int)\n",
    "\n",
    "print(f\"\\nâœ“ One-Hot Encoding applied\")\n",
    "print(f\"Shape before encoding: {df_processed.shape}\")\n",
    "print(f\"Shape after encoding: {df_encoded.shape}\")\n",
    "print(f\"New features created: {df_encoded.shape[1] - df_processed.shape[1]}\")\n",
    "\n",
    "# Display sample of encoded columns\n",
    "print(\"\\nSample of new encoded columns (first 5 rows, showing some encoded features):\")\n",
    "encoded_cols = [col for col in df_encoded.columns if any(cat in col for cat in categorical_features)]\n",
    "print(df_encoded[encoded_cols[:10]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d76175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPARING FEATURES AND TARGET\n",
      "================================================================================\n",
      "Features (X) shape: (200000, 57)\n",
      "Target (y) shape: (200000,)\n",
      "\n",
      "Target distribution:\n",
      "Is_Fraud\n",
      "0    189912\n",
      "1     10088\n",
      "Name: count, dtype: int64\n",
      "Fraud percentage: 5.04%\n",
      "\n",
      "Total features for modeling: 57\n",
      "\n",
      "Feature categories breakdown:\n",
      "  â€¢ Time-based features: 7\n",
      "  â€¢ Transaction features: 8\n",
      "  â€¢ Frequency encoded features: 3\n",
      "  â€¢ One-hot encoded features: 38\n",
      "  â€¢ Other features: 1\n",
      "\n",
      "First 15 feature names:\n",
      "  1. Age\n",
      "  2. State\n",
      "  3. City\n",
      "  4. Transaction_Amount\n",
      "  5. Account_Balance\n",
      "  6. Transaction_Description\n",
      "  7. Hour\n",
      "  8. Day_of_Week\n",
      "  9. Day\n",
      "  10. Month\n",
      "  11. Year\n",
      "  12. Is_Weekend\n",
      "  13. Is_Night\n",
      "  14. Is_Business_Hours\n",
      "  15. Transaction_to_Balance_Ratio\n",
      "  16. Is_High_Value\n",
      "  17. Is_Low_Balance\n",
      "  18. State_Frequency\n",
      "  19. City_Frequency\n",
      "  20. Transaction_Description_Frequency\n",
      "  21. Gender_Male\n",
      "  22. Account_Type_Checking\n",
      "  23. Account_Type_Savings\n",
      "  24. Transaction_Type_Credit\n",
      "  25. Transaction_Type_Debit\n",
      "  26. Transaction_Type_Transfer\n",
      "  27. Transaction_Type_Withdrawal\n",
      "  28. Merchant_Category_Electronics\n",
      "  29. Merchant_Category_Entertainment\n",
      "  30. Merchant_Category_Groceries\n",
      "  31. Merchant_Category_Health\n",
      "  32. Merchant_Category_Restaurant\n",
      "  33. Transaction_Device_ATM Booth Kiosk\n",
      "  34. Transaction_Device_Bank Branch\n",
      "  35. Transaction_Device_Banking Chatbot\n",
      "  36. Transaction_Device_Biometric Scanner\n",
      "  37. Transaction_Device_Debit/Credit Card\n",
      "  38. Transaction_Device_Desktop/Laptop\n",
      "  39. Transaction_Device_Mobile Device\n",
      "  40. Transaction_Device_POS Mobile App\n",
      "  41. Transaction_Device_POS Mobile Device\n",
      "  42. Transaction_Device_POS Terminal\n",
      "  43. Transaction_Device_Payment Gateway Device\n",
      "  44. Transaction_Device_QR Code Scanner\n",
      "  45. Transaction_Device_Self-service Banking Machine\n",
      "  46. Transaction_Device_Smart Card\n",
      "  47. Transaction_Device_Tablet\n",
      "  48. Transaction_Device_Virtual Card\n",
      "  49. Transaction_Device_Voice Assistant\n",
      "  50. Transaction_Device_Wearable Device\n",
      "  51. Transaction_Device_Web Browser\n",
      "  52. Device_Type_Desktop\n",
      "  53. Device_Type_Mobile\n",
      "  54. Device_Type_POS\n",
      "  55. Amount_Category_Medium\n",
      "  56. Amount_Category_High\n",
      "  57. Amount_Category_Very High\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PREPARING FEATURES AND TARGET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_model = df_encoded.drop(columns=drop_features, errors='ignore')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_model.drop(columns=[target])\n",
    "y = df_model[target]\n",
    "\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Fraud percentage: {y.mean()*100:.2f}%\")\n",
    "\n",
    "# Display feature names\n",
    "print(f\"\\nTotal features for modeling: {X.shape[1]}\")\n",
    "print(\"\\nFeature categories breakdown:\")\n",
    "\n",
    "# Count different types of features\n",
    "time_features = [col for col in X.columns if any(word in col.lower() for word in ['hour', 'day', 'month', 'weekend', 'night', 'business'])]\n",
    "transaction_features = [col for col in X.columns if any(word in col.lower() for word in ['amount', 'balance', 'ratio', 'high', 'low'])]\n",
    "encoded_features = [col for col in X.columns if '_' in col and col not in time_features + transaction_features]\n",
    "frequency_features = [col for col in X.columns if 'frequency' in col.lower()]\n",
    "\n",
    "print(f\"  â€¢ Time-based features: {len(time_features)}\")\n",
    "print(f\"  â€¢ Transaction features: {len(transaction_features)}\")\n",
    "print(f\"  â€¢ Frequency encoded features: {len(frequency_features)}\")\n",
    "print(f\"  â€¢ One-hot encoded features: {len(encoded_features)}\")\n",
    "print(f\"  â€¢ Other features: {X.shape[1] - len(time_features) - len(transaction_features) - len(frequency_features) - len(encoded_features)}\")\n",
    "\n",
    "print(\"\\nFirst 15 feature names:\")\n",
    "for i, col in enumerate(X.columns[:57], 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d3f63ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REMOVING ORIGINAL HIGH CARDINALITY FEATURES\n",
      "================================================================================\n",
      "Removing original high cardinality features: ['State', 'City', 'Transaction_Description']\n",
      "(We already have their frequency-encoded versions)\n",
      "  âœ“ Removed State\n",
      "  âœ“ Removed City\n",
      "  âœ“ Removed Transaction_Description\n",
      "\n",
      "Updated feature count: 54\n",
      "\n",
      "First 20 features after cleanup:\n",
      "  1. Age\n",
      "  2. Transaction_Amount\n",
      "  3. Account_Balance\n",
      "  4. Hour\n",
      "  5. Day_of_Week\n",
      "  6. Day\n",
      "  7. Month\n",
      "  8. Year\n",
      "  9. Is_Weekend\n",
      "  10. Is_Night\n",
      "  11. Is_Business_Hours\n",
      "  12. Transaction_to_Balance_Ratio\n",
      "  13. Is_High_Value\n",
      "  14. Is_Low_Balance\n",
      "  15. State_Frequency\n",
      "  16. City_Frequency\n",
      "  17. Transaction_Description_Frequency\n",
      "  18. Gender_Male\n",
      "  19. Account_Type_Checking\n",
      "  20. Account_Type_Savings\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"REMOVING ORIGINAL HIGH CARDINALITY FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# These were already converted to frequency encoding, so we should drop the originals\n",
    "high_cardinality_originals = ['State', 'City', 'Transaction_Description']\n",
    "\n",
    "print(f\"Removing original high cardinality features: {high_cardinality_originals}\")\n",
    "print(f\"(We already have their frequency-encoded versions)\")\n",
    "\n",
    "# Check if they exist and remove them\n",
    "for col in high_cardinality_originals:\n",
    "    if col in X.columns:\n",
    "        X = X.drop(columns=[col])\n",
    "        print(f\"  âœ“ Removed {col}\")\n",
    "\n",
    "print(f\"\\nUpdated feature count: {X.shape[1]}\")\n",
    "print(f\"\\nFirst 20 features after cleanup:\")\n",
    "for i, col in enumerate(X.columns[:20], 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca6a6154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAIN-TEST SPLIT (Hold-out Test Set)\n",
      "================================================================================\n",
      "Total samples: 200,000\n",
      "\n",
      "Data for K-Fold CV: 160,000 samples (80%)\n",
      "Hold-out test set: 40,000 samples (20%)\n",
      "\n",
      "==================================================\n",
      "K-Fold CV Data (Training Portion):\n",
      "==================================================\n",
      "Total samples: 160,000\n",
      "Is_Fraud\n",
      "0    151930\n",
      "1      8070\n",
      "Name: count, dtype: int64\n",
      "Fraud rate: 5.04%\n",
      "\n",
      "==================================================\n",
      "Hold-out Test Set (Final Evaluation):\n",
      "==================================================\n",
      "Total samples: 40,000\n",
      "Is_Fraud\n",
      "0    37982\n",
      "1     2018\n",
      "Name: count, dtype: int64\n",
      "Fraud rate: 5.04%\n",
      "\n",
      "==================================================\n",
      "Stratification Verification:\n",
      "==================================================\n",
      "Original fraud rate:    5.04%\n",
      "K-Fold data fraud rate: 5.04%\n",
      "Test set fraud rate:    5.04%\n",
      "\n",
      "âœ“ Stratification successful - fraud rates match!\n",
      "\n",
      "ðŸ“Œ Important Notes:\n",
      "  â€¢ The hold-out test set will NOT be touched until final evaluation\n",
      "  â€¢ K-Fold CV will be performed on the training portion (160,000 samples)\n",
      "  â€¢ SMOTE and scaling will be applied INSIDE each fold\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAIN-TEST SPLIT (Hold-out Test Set)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split: 80% for K-Fold CV, 20% held-out for final testing\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,           # 20% for final testing\n",
    "    random_state=42,         \n",
    "    stratify=y               # CRITICAL: Maintain fraud ratio\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {len(X):,}\")\n",
    "print(f\"\\nData for K-Fold CV: {X_train_full.shape[0]:,} samples ({X_train_full.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"Hold-out test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"K-Fold CV Data (Training Portion):\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total samples: {len(y_train_full):,}\")\n",
    "print(y_train_full.value_counts())\n",
    "print(f\"Fraud rate: {y_train_full.mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Hold-out Test Set (Final Evaluation):\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total samples: {len(y_test):,}\")\n",
    "print(y_test.value_counts())\n",
    "print(f\"Fraud rate: {y_test.mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Stratification Verification:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Original fraud rate:    {y.mean()*100:.2f}%\")\n",
    "print(f\"K-Fold data fraud rate: {y_train_full.mean()*100:.2f}%\")\n",
    "print(f\"Test set fraud rate:    {y_test.mean()*100:.2f}%\")\n",
    "print(\"\\nâœ“ Stratification successful - fraud rates match!\")\n",
    "\n",
    "print(\"\\nðŸ“Œ Important Notes:\")\n",
    "print(\"  â€¢ The hold-out test set will NOT be touched until final evaluation\")\n",
    "print(\"  â€¢ K-Fold CV will be performed on the training portion (160,000 samples)\")\n",
    "print(\"  â€¢ SMOTE and scaling will be applied INSIDE each fold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aff7c8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRATIFIED K-FOLD CROSS-VALIDATION STRATEGY\n",
      "================================================================================\n",
      "Strategy: 5-Fold Stratified Cross-Validation\n",
      "\n",
      "How it works:\n",
      "  1. Split 160,000 samples into 5 folds\n",
      "  2. Each fold maintains ~5.04% fraud rate\n",
      "  3. For each fold:\n",
      "     â€¢ Train on 4 folds (~128000 samples)\n",
      "     â€¢ Validate on 1 fold (~32000 samples)\n",
      "     â€¢ Apply SMOTE to training folds only\n",
      "     â€¢ Scale features on training folds, apply to validation\n",
      "     â€¢ Train model and evaluate on validation fold\n",
      "  4. Average performance across all 5 folds\n",
      "  5. Final model trained on all 160,000 samples\n",
      "  6. Final evaluation on hold-out test set (40,000 samples)\n",
      "\n",
      "================================================================================\n",
      "FOLD BREAKDOWN - Verification of Stratification:\n",
      "================================================================================\n",
      "\n",
      "Fold 1:\n",
      "  Training:   128,000 samples | Fraud: 6,456 ( 5.04%)\n",
      "  Validation:  32,000 samples | Fraud: 1,614 ( 5.04%)\n",
      "\n",
      "Fold 2:\n",
      "  Training:   128,000 samples | Fraud: 6,456 ( 5.04%)\n",
      "  Validation:  32,000 samples | Fraud: 1,614 ( 5.04%)\n",
      "\n",
      "Fold 3:\n",
      "  Training:   128,000 samples | Fraud: 6,456 ( 5.04%)\n",
      "  Validation:  32,000 samples | Fraud: 1,614 ( 5.04%)\n",
      "\n",
      "Fold 4:\n",
      "  Training:   128,000 samples | Fraud: 6,456 ( 5.04%)\n",
      "  Validation:  32,000 samples | Fraud: 1,614 ( 5.04%)\n",
      "\n",
      "Fold 5:\n",
      "  Training:   128,000 samples | Fraud: 6,456 ( 5.04%)\n",
      "  Validation:  32,000 samples | Fraud: 1,614 ( 5.04%)\n",
      "\n",
      "================================================================================\n",
      "âœ“ All folds maintain consistent ~5.04% fraud rate!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STRATIFIED K-FOLD CROSS-VALIDATION STRATEGY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Initialize Stratified K-Fold\n",
    "n_splits = 5  # We'll use 5 folds\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Strategy: {n_splits}-Fold Stratified Cross-Validation\")\n",
    "print(f\"\\nHow it works:\")\n",
    "print(f\"  1. Split {X_train_full.shape[0]:,} samples into {n_splits} folds\")\n",
    "print(f\"  2. Each fold maintains ~{y_train_full.mean()*100:.2f}% fraud rate\")\n",
    "print(f\"  3. For each fold:\")\n",
    "print(f\"     â€¢ Train on {n_splits-1} folds (~{X_train_full.shape[0]*(n_splits-1)/n_splits:.0f} samples)\")\n",
    "print(f\"     â€¢ Validate on 1 fold (~{X_train_full.shape[0]/n_splits:.0f} samples)\")\n",
    "print(f\"     â€¢ Apply SMOTE to training folds only\")\n",
    "print(f\"     â€¢ Scale features on training folds, apply to validation\")\n",
    "print(f\"     â€¢ Train model and evaluate on validation fold\")\n",
    "print(f\"  4. Average performance across all {n_splits} folds\")\n",
    "print(f\"  5. Final model trained on all {X_train_full.shape[0]:,} samples\")\n",
    "print(f\"  6. Final evaluation on hold-out test set ({X_test.shape[0]:,} samples)\")\n",
    "\n",
    "# Demonstrate the splits\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FOLD BREAKDOWN - Verification of Stratification:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train_full, y_train_full), 1):\n",
    "    y_train_fold = y_train_full.iloc[train_idx]\n",
    "    y_val_fold = y_train_full.iloc[val_idx]\n",
    "    \n",
    "    fraud_train = y_train_fold.sum()\n",
    "    fraud_val = y_val_fold.sum()\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx}:\")\n",
    "    print(f\"  Training:   {len(train_idx):>7,} samples | Fraud: {fraud_train:>5,} ({y_train_fold.mean()*100:>5.2f}%)\")\n",
    "    print(f\"  Validation: {len(val_idx):>7,} samples | Fraud: {fraud_val:>5,} ({y_val_fold.mean()*100:>5.2f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ“ All folds maintain consistent ~5.04% fraud rate!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0487276b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAVING PREPROCESSED DATA\n",
      "================================================================================\n",
      "Saving files...\n",
      "\n",
      "âœ“ All files saved successfully!\n",
      "\n",
      "Saved files in '../data/preprocessed/':\n",
      "  1. X_train_full.npy         - Training data for K-Fold CV (160,000 samples)\n",
      "  2. y_train_full.npy         - Training labels\n",
      "  3. X_test.npy               - Hold-out test set (40,000 samples)\n",
      "  4. X_test_scaled.npy        - Scaled hold-out test set\n",
      "  5. y_test.npy               - Test labels\n",
      "  6. scaler.pkl               - Fitted StandardScaler\n",
      "  7. feature_names.txt        - List of all 54 features\n",
      "  8. preprocessing_params.pkl - All preprocessing parameters\n",
      "\n",
      "File sizes:\n",
      "  â€¢ feature_names.txt: 0.00 MB\n",
      "  â€¢ preprocessing_params.pkl: 0.00 MB\n",
      "  â€¢ scaler.pkl: 0.00 MB\n",
      "  â€¢ X_test.npy: 16.48 MB\n",
      "  â€¢ X_test_scaled.npy: 16.48 MB\n",
      "  â€¢ X_train_full.npy: 65.92 MB\n",
      "  â€¢ y_test.npy: 0.31 MB\n",
      "  â€¢ y_train_full.npy: 1.22 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SAVING PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create directory for preprocessed data\n",
    "os.makedirs('../data/preprocessed', exist_ok=True)\n",
    "\n",
    "# Fit scaler on full training data (we'll refit inside CV, but save this for deployment)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_full)\n",
    "\n",
    "# Scale the hold-out test set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "# Save the data\n",
    "print(\"Saving files...\")\n",
    "\n",
    "# Training data for K-Fold CV (unscaled - we'll scale inside each fold)\n",
    "np.save('../data/preprocessed/X_train_full.npy', X_train_full.values)\n",
    "np.save('../data/preprocessed/y_train_full.npy', y_train_full.values)\n",
    "\n",
    "# Hold-out test set\n",
    "np.save('../data/preprocessed/X_test.npy', X_test.values)\n",
    "np.save('../data/preprocessed/X_test_scaled.npy', X_test_scaled.values)\n",
    "np.save('../data/preprocessed/y_test.npy', y_test.values)\n",
    "\n",
    "# Save the scaler for deployment\n",
    "joblib.dump(scaler, '../data/preprocessed/scaler.pkl')\n",
    "\n",
    "# Save feature names\n",
    "with open('../data/preprocessed/feature_names.txt', 'w') as f:\n",
    "    for col in X.columns:\n",
    "        f.write(f\"{col}\\n\")\n",
    "\n",
    "# Save preprocessing parameters\n",
    "params = {\n",
    "    'n_folds': n_splits,\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'smote_strategy': 0.5,\n",
    "    'total_features': X.shape[1],\n",
    "    'feature_names': list(X.columns)\n",
    "}\n",
    "joblib.dump(params, '../data/preprocessed/preprocessing_params.pkl')\n",
    "\n",
    "print(\"\\nâœ“ All files saved successfully!\")\n",
    "print(\"\\nSaved files in '../data/preprocessed/':\")\n",
    "print(\"  1. X_train_full.npy         - Training data for K-Fold CV (160,000 samples)\")\n",
    "print(\"  2. y_train_full.npy         - Training labels\")\n",
    "print(\"  3. X_test.npy               - Hold-out test set (40,000 samples)\")\n",
    "print(\"  4. X_test_scaled.npy        - Scaled hold-out test set\")\n",
    "print(\"  5. y_test.npy               - Test labels\")\n",
    "print(\"  6. scaler.pkl               - Fitted StandardScaler\")\n",
    "print(\"  7. feature_names.txt        - List of all 54 features\")\n",
    "print(\"  8. preprocessing_params.pkl - All preprocessing parameters\")\n",
    "\n",
    "print(f\"\\nFile sizes:\")\n",
    "import os\n",
    "for filename in os.listdir('../data/preprocessed/'):\n",
    "    filepath = f'../data/preprocessed/{filename}'\n",
    "    size = os.path.getsize(filepath) / (1024 * 1024)  # Convert to MB\n",
    "    print(f\"  â€¢ {filename}: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0657beb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPROCESSING COMPLETE - FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š ORIGINAL DATA:\n",
      "  â€¢ Total samples: 200,000\n",
      "  â€¢ Original features: 40\n",
      "  â€¢ Fraud cases: 10,088 (5.04%)\n",
      "\n",
      "ðŸ”§ FEATURE ENGINEERING:\n",
      "  â€¢ Time-based features created: 6\n",
      "    (Hour, Day, Month, Is_Weekend, Is_Night, Is_Business_Hours)\n",
      "  â€¢ Transaction features created: 4\n",
      "    (Transaction_to_Balance_Ratio, Is_High_Value, Is_Low_Balance, Amount_Category)\n",
      "  â€¢ Frequency encoding applied: 3\n",
      "    (State_Frequency, City_Frequency, Transaction_Description_Frequency)\n",
      "\n",
      "ðŸ“ ENCODING:\n",
      "  â€¢ Categorical features one-hot encoded: 7\n",
      "  â€¢ Total features after encoding: 54\n",
      "\n",
      "âœ‚ï¸ DATA SPLIT:\n",
      "  â€¢ K-Fold CV data (80%): 160,000 samples\n",
      "    - Fraud: 8,070 (5.04%)\n",
      "  â€¢ Hold-out test set (20%): 40,000 samples\n",
      "    - Fraud: 2,018 (5.04%)\n",
      "\n",
      "ðŸ”„ CROSS-VALIDATION STRATEGY:\n",
      "  â€¢ Method: 5-Fold Stratified Cross-Validation\n",
      "  â€¢ Training per fold: 128,000 samples\n",
      "  â€¢ Validation per fold: 32,000 samples\n",
      "  â€¢ SMOTE strategy: 0.5 (increase fraud to 50% of non-fraud)\n",
      "  â€¢ Scaling: StandardScaler (applied inside each fold)\n",
      "\n",
      "ðŸ’¾ SAVED FILES:\n",
      "  â€¢ Location: ../data/preprocessed/\n",
      "  â€¢ Total size: ~100 MB\n",
      "  â€¢ Files: 8 (data arrays, scaler, parameters, feature names)\n",
      "\n",
      "âœ… PREPROCESSING COMPLETE!\n",
      "\n",
      "================================================================================\n",
      "READY FOR MODEL TRAINING!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PREPROCESSING COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“Š ORIGINAL DATA:\")\n",
    "print(f\"  â€¢ Total samples: {len(df):,}\")\n",
    "print(f\"  â€¢ Original features: {df.shape[1]}\")\n",
    "print(f\"  â€¢ Fraud cases: {df['Is_Fraud'].sum():,} ({df['Is_Fraud'].mean()*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nðŸ”§ FEATURE ENGINEERING:\")\n",
    "print(f\"  â€¢ Time-based features created: 6\")\n",
    "print(f\"    (Hour, Day, Month, Is_Weekend, Is_Night, Is_Business_Hours)\")\n",
    "print(f\"  â€¢ Transaction features created: 4\")\n",
    "print(f\"    (Transaction_to_Balance_Ratio, Is_High_Value, Is_Low_Balance, Amount_Category)\")\n",
    "print(f\"  â€¢ Frequency encoding applied: 3\")\n",
    "print(f\"    (State_Frequency, City_Frequency, Transaction_Description_Frequency)\")\n",
    "\n",
    "print(\"\\nðŸ“ ENCODING:\")\n",
    "print(f\"  â€¢ Categorical features one-hot encoded: 7\")\n",
    "print(f\"  â€¢ Total features after encoding: {X.shape[1]}\")\n",
    "\n",
    "print(\"\\nâœ‚ï¸ DATA SPLIT:\")\n",
    "print(f\"  â€¢ K-Fold CV data (80%): {len(X_train_full):,} samples\")\n",
    "print(f\"    - Fraud: {y_train_full.sum():,} ({y_train_full.mean()*100:.2f}%)\")\n",
    "print(f\"  â€¢ Hold-out test set (20%): {len(X_test):,} samples\")\n",
    "print(f\"    - Fraud: {y_test.sum():,} ({y_test.mean()*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nðŸ”„ CROSS-VALIDATION STRATEGY:\")\n",
    "print(f\"  â€¢ Method: {n_splits}-Fold Stratified Cross-Validation\")\n",
    "print(f\"  â€¢ Training per fold: {X_train_full.shape[0]*(n_splits-1)//n_splits:,} samples\")\n",
    "print(f\"  â€¢ Validation per fold: {X_train_full.shape[0]//n_splits:,} samples\")\n",
    "print(f\"  â€¢ SMOTE strategy: 0.5 (increase fraud to 50% of non-fraud)\")\n",
    "print(f\"  â€¢ Scaling: StandardScaler (applied inside each fold)\")\n",
    "\n",
    "print(\"\\nðŸ’¾ SAVED FILES:\")\n",
    "print(f\"  â€¢ Location: ../data/preprocessed/\")\n",
    "print(f\"  â€¢ Total size: ~100 MB\")\n",
    "print(f\"  â€¢ Files: 8 (data arrays, scaler, parameters, feature names)\")\n",
    "\n",
    "print(\"\\nâœ… PREPROCESSING COMPLETE!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"READY FOR MODEL TRAINING!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
