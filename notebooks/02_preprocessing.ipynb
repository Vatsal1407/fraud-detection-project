{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251c6e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# For handling imbalanced data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac6b61b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 200,000 rows, 24 columns\n",
      "Fraud cases: 10,088 (5.04%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_ID</th>\n",
       "      <th>Customer_Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>Bank_Branch</th>\n",
       "      <th>Account_Type</th>\n",
       "      <th>Transaction_ID</th>\n",
       "      <th>Transaction_Date</th>\n",
       "      <th>Transaction_Time</th>\n",
       "      <th>Transaction_Amount</th>\n",
       "      <th>Merchant_ID</th>\n",
       "      <th>Transaction_Type</th>\n",
       "      <th>Merchant_Category</th>\n",
       "      <th>Account_Balance</th>\n",
       "      <th>Transaction_Device</th>\n",
       "      <th>Transaction_Location</th>\n",
       "      <th>Device_Type</th>\n",
       "      <th>Is_Fraud</th>\n",
       "      <th>Transaction_Currency</th>\n",
       "      <th>Customer_Contact</th>\n",
       "      <th>Transaction_Description</th>\n",
       "      <th>Customer_Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d5f6ec07-d69e-4f47-b9b4-7c58ff17c19e</td>\n",
       "      <td>Osha Tella</td>\n",
       "      <td>Male</td>\n",
       "      <td>60</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>Thiruvananthapuram</td>\n",
       "      <td>Thiruvananthapuram Branch</td>\n",
       "      <td>Savings</td>\n",
       "      <td>4fa3208f-9e23-42dc-b330-844829d0c12c</td>\n",
       "      <td>23-01-2025</td>\n",
       "      <td>16:04:07</td>\n",
       "      <td>32415.45</td>\n",
       "      <td>214e03c5-5c34-40d1-a66c-f440aa2bbd02</td>\n",
       "      <td>Transfer</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>74557.27</td>\n",
       "      <td>Voice Assistant</td>\n",
       "      <td>Thiruvananthapuram, Kerala</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>INR</td>\n",
       "      <td>+9198579XXXXXX</td>\n",
       "      <td>Bitcoin transaction</td>\n",
       "      <td>oshaXXXXX@XXXXX.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7c14ad51-781a-4db9-b7bd-67439c175262</td>\n",
       "      <td>Hredhaan Khosla</td>\n",
       "      <td>Female</td>\n",
       "      <td>51</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>Nashik</td>\n",
       "      <td>Nashik Branch</td>\n",
       "      <td>Business</td>\n",
       "      <td>c9de0c06-2c4c-40a9-97ed-3c7b8f97c79c</td>\n",
       "      <td>11-01-2025</td>\n",
       "      <td>17:14:53</td>\n",
       "      <td>43622.60</td>\n",
       "      <td>f9e3f11f-28d3-4199-b0ca-f225a155ede6</td>\n",
       "      <td>Bill Payment</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>74622.66</td>\n",
       "      <td>POS Mobile Device</td>\n",
       "      <td>Nashik, Maharashtra</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>0</td>\n",
       "      <td>INR</td>\n",
       "      <td>+9191074XXXXXX</td>\n",
       "      <td>Grocery delivery</td>\n",
       "      <td>hredhaanXXXX@XXXXXX.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3a73a0e5-d4da-45aa-85f3-528413900a35</td>\n",
       "      <td>Ekani Nazareth</td>\n",
       "      <td>Male</td>\n",
       "      <td>20</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>Bhagalpur</td>\n",
       "      <td>Bhagalpur Branch</td>\n",
       "      <td>Savings</td>\n",
       "      <td>e41c55f9-c016-4ff3-872b-cae72467c75c</td>\n",
       "      <td>25-01-2025</td>\n",
       "      <td>03:09:52</td>\n",
       "      <td>63062.56</td>\n",
       "      <td>97977d83-5486-4510-af1c-8dada3e1cfa0</td>\n",
       "      <td>Bill Payment</td>\n",
       "      <td>Groceries</td>\n",
       "      <td>66817.99</td>\n",
       "      <td>ATM</td>\n",
       "      <td>Bhagalpur, Bihar</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>0</td>\n",
       "      <td>INR</td>\n",
       "      <td>+9197745XXXXXX</td>\n",
       "      <td>Mutual fund investment</td>\n",
       "      <td>ekaniXXX@XXXXXX.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7902f4ef-9050-4a79-857d-9c2ea3181940</td>\n",
       "      <td>Yamini Ramachandran</td>\n",
       "      <td>Female</td>\n",
       "      <td>57</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Chennai Branch</td>\n",
       "      <td>Business</td>\n",
       "      <td>7f7ee11b-ff2c-45a3-802a-49bc47c02ecb</td>\n",
       "      <td>19-01-2025</td>\n",
       "      <td>12:27:02</td>\n",
       "      <td>14000.72</td>\n",
       "      <td>f45cd6b3-5092-44d0-8afb-490894605184</td>\n",
       "      <td>Debit</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>58177.08</td>\n",
       "      <td>POS Mobile App</td>\n",
       "      <td>Chennai, Tamil Nadu</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>0</td>\n",
       "      <td>INR</td>\n",
       "      <td>+9195889XXXXXX</td>\n",
       "      <td>Food delivery</td>\n",
       "      <td>yaminiXXXXX@XXXXXXX.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3a4bba70-d9a9-4c5f-8b92-1735fd8c19e9</td>\n",
       "      <td>Kritika Rege</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>Amritsar</td>\n",
       "      <td>Amritsar Branch</td>\n",
       "      <td>Savings</td>\n",
       "      <td>f8e6ac6f-81a1-4985-bf12-f60967d852ef</td>\n",
       "      <td>30-01-2025</td>\n",
       "      <td>18:30:46</td>\n",
       "      <td>18335.16</td>\n",
       "      <td>70dd77dd-3b00-4b2c-8ebc-cfb8af5f6741</td>\n",
       "      <td>Transfer</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>16108.56</td>\n",
       "      <td>Virtual Card</td>\n",
       "      <td>Amritsar, Punjab</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>0</td>\n",
       "      <td>INR</td>\n",
       "      <td>+9195316XXXXXX</td>\n",
       "      <td>Debt repayment</td>\n",
       "      <td>kritikaXXXX@XXXXXX.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Customer_ID        Customer_Name  Gender  Age  \\\n",
       "0  d5f6ec07-d69e-4f47-b9b4-7c58ff17c19e           Osha Tella    Male   60   \n",
       "1  7c14ad51-781a-4db9-b7bd-67439c175262      Hredhaan Khosla  Female   51   \n",
       "2  3a73a0e5-d4da-45aa-85f3-528413900a35       Ekani Nazareth    Male   20   \n",
       "3  7902f4ef-9050-4a79-857d-9c2ea3181940  Yamini Ramachandran  Female   57   \n",
       "4  3a4bba70-d9a9-4c5f-8b92-1735fd8c19e9         Kritika Rege  Female   43   \n",
       "\n",
       "         State                City                Bank_Branch Account_Type  \\\n",
       "0       Kerala  Thiruvananthapuram  Thiruvananthapuram Branch      Savings   \n",
       "1  Maharashtra              Nashik              Nashik Branch     Business   \n",
       "2        Bihar           Bhagalpur           Bhagalpur Branch      Savings   \n",
       "3   Tamil Nadu             Chennai             Chennai Branch     Business   \n",
       "4       Punjab            Amritsar            Amritsar Branch      Savings   \n",
       "\n",
       "                         Transaction_ID Transaction_Date Transaction_Time  \\\n",
       "0  4fa3208f-9e23-42dc-b330-844829d0c12c       23-01-2025         16:04:07   \n",
       "1  c9de0c06-2c4c-40a9-97ed-3c7b8f97c79c       11-01-2025         17:14:53   \n",
       "2  e41c55f9-c016-4ff3-872b-cae72467c75c       25-01-2025         03:09:52   \n",
       "3  7f7ee11b-ff2c-45a3-802a-49bc47c02ecb       19-01-2025         12:27:02   \n",
       "4  f8e6ac6f-81a1-4985-bf12-f60967d852ef       30-01-2025         18:30:46   \n",
       "\n",
       "   Transaction_Amount                           Merchant_ID Transaction_Type  \\\n",
       "0            32415.45  214e03c5-5c34-40d1-a66c-f440aa2bbd02         Transfer   \n",
       "1            43622.60  f9e3f11f-28d3-4199-b0ca-f225a155ede6     Bill Payment   \n",
       "2            63062.56  97977d83-5486-4510-af1c-8dada3e1cfa0     Bill Payment   \n",
       "3            14000.72  f45cd6b3-5092-44d0-8afb-490894605184            Debit   \n",
       "4            18335.16  70dd77dd-3b00-4b2c-8ebc-cfb8af5f6741         Transfer   \n",
       "\n",
       "  Merchant_Category  Account_Balance Transaction_Device  \\\n",
       "0        Restaurant         74557.27    Voice Assistant   \n",
       "1        Restaurant         74622.66  POS Mobile Device   \n",
       "2         Groceries         66817.99                ATM   \n",
       "3     Entertainment         58177.08     POS Mobile App   \n",
       "4     Entertainment         16108.56       Virtual Card   \n",
       "\n",
       "         Transaction_Location Device_Type  Is_Fraud Transaction_Currency  \\\n",
       "0  Thiruvananthapuram, Kerala         POS         0                  INR   \n",
       "1         Nashik, Maharashtra     Desktop         0                  INR   \n",
       "2            Bhagalpur, Bihar     Desktop         0                  INR   \n",
       "3         Chennai, Tamil Nadu      Mobile         0                  INR   \n",
       "4            Amritsar, Punjab      Mobile         0                  INR   \n",
       "\n",
       "  Customer_Contact Transaction_Description           Customer_Email  \n",
       "0   +9198579XXXXXX     Bitcoin transaction      oshaXXXXX@XXXXX.com  \n",
       "1   +9191074XXXXXX        Grocery delivery  hredhaanXXXX@XXXXXX.com  \n",
       "2   +9197745XXXXXX  Mutual fund investment      ekaniXXX@XXXXXX.com  \n",
       "3   +9195889XXXXXX           Food delivery  yaminiXXXXX@XXXXXXX.com  \n",
       "4   +9195316XXXXXX          Debt repayment   kritikaXXXX@XXXXXX.com  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('../data/Bank_Transaction_Fraud_Detection.csv')\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "print(f\"Fraud cases: {df['Is_Fraud'].sum():,} ({df['Is_Fraud'].mean()*100:.2f}%)\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59002b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING: TIME-BASED FEATURES\n",
      "================================================================================\n",
      "Time-based features created:\n",
      "  ‚Ä¢ Hour (0-23)\n",
      "  ‚Ä¢ Day_of_Week (0=Mon, 6=Sun)\n",
      "  ‚Ä¢ Day, Month, Year\n",
      "  ‚Ä¢ Is_Weekend (0/1)\n",
      "  ‚Ä¢ Is_Night (0/1)\n",
      "  ‚Ä¢ Is_Business_Hours (0/1)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING: TIME-BASED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Parse datetime\n",
    "df['Transaction_DateTime'] = pd.to_datetime(df['Transaction_Date'] + ' ' + df['Transaction_Time'], \n",
    "                                             format='%d-%m-%Y %H:%M:%S')\n",
    "\n",
    "# Extract time components\n",
    "df['Hour'] = df['Transaction_DateTime'].dt.hour\n",
    "df['Day_of_Week'] = df['Transaction_DateTime'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "df['Day'] = df['Transaction_DateTime'].dt.day\n",
    "df['Month'] = df['Transaction_DateTime'].dt.month\n",
    "df['Year'] = df['Transaction_DateTime'].dt.year\n",
    "\n",
    "# Create time-based categories\n",
    "df['Is_Weekend'] = df['Day_of_Week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "df['Is_Night'] = df['Hour'].apply(lambda x: 1 if x >= 22 or x <= 6 else 0)\n",
    "df['Is_Business_Hours'] = df['Hour'].apply(lambda x: 1 if 9 <= x <= 17 else 0)\n",
    "\n",
    "print(\"Time-based features created:\")\n",
    "print(\"  ‚Ä¢ Hour (0-23)\")\n",
    "print(\"  ‚Ä¢ Day_of_Week (0=Mon, 6=Sun)\")\n",
    "print(\"  ‚Ä¢ Day, Month, Year\")\n",
    "print(\"  ‚Ä¢ Is_Weekend (0/1)\")\n",
    "print(\"  ‚Ä¢ Is_Night (0/1)\")\n",
    "print(\"  ‚Ä¢ Is_Business_Hours (0/1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9531764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING: TRANSACTION-BASED FEATURES\n",
      "================================================================================\n",
      "Transaction features created:\n",
      "  ‚Ä¢ Transaction_to_Balance_Ratio\n",
      "  ‚Ä¢ Amount_Category (Low/Medium/High/Very High)\n",
      "\n",
      "Note: Is_High_Value and Is_Low_Balance will be created AFTER train-test split\n",
      "      to prevent data leakage from quantile calculations\n",
      "\n",
      "Sample of new features:\n",
      "   Transaction_Amount  Account_Balance  Transaction_to_Balance_Ratio  \\\n",
      "0            32415.45         74557.27                      0.434772   \n",
      "1            43622.60         74622.66                      0.584576   \n",
      "2            63062.56         66817.99                      0.943796   \n",
      "3            14000.72         58177.08                      0.240657   \n",
      "4            18335.16         16108.56                      1.138225   \n",
      "5             9711.15         61258.85                      0.158526   \n",
      "6            94677.01         36313.61                      2.607205   \n",
      "7            67704.28         16948.73                      3.994652   \n",
      "8            72953.45         18138.71                      4.021976   \n",
      "9             5689.02         65801.35                      0.086457   \n",
      "\n",
      "  Amount_Category  \n",
      "0          Medium  \n",
      "1          Medium  \n",
      "2            High  \n",
      "3             Low  \n",
      "4             Low  \n",
      "5             Low  \n",
      "6       Very High  \n",
      "7            High  \n",
      "8            High  \n",
      "9             Low  \n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING: TRANSACTION-BASED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Transaction to Balance Ratio\n",
    "df['Transaction_to_Balance_Ratio'] = df['Transaction_Amount'] / df['Account_Balance']\n",
    "\n",
    "# Amount Category (using fixed thresholds - no leakage)\n",
    "df['Amount_Category'] = pd.cut(df['Transaction_Amount'], \n",
    "                                bins=[0, 25000, 50000, 75000, 100000],\n",
    "                                labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "print(\"Transaction features created:\")\n",
    "print(f\"  ‚Ä¢ Transaction_to_Balance_Ratio\")\n",
    "print(f\"  ‚Ä¢ Amount_Category (Low/Medium/High/Very High)\")\n",
    "print(\"\\nNote: Is_High_Value and Is_Low_Balance will be created AFTER train-test split\")\n",
    "print(\"      to prevent data leakage from quantile calculations\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of new features:\")\n",
    "print(df[['Transaction_Amount', 'Account_Balance', 'Transaction_to_Balance_Ratio', \n",
    "          'Amount_Category']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f1c2865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE SELECTION\n",
      "================================================================================\n",
      "Features to DROP: 12\n",
      "Categorical features: 10\n",
      "Numerical features: 13\n",
      "Target: Is_Fraud\n",
      "\n",
      "Total features for modeling: 23\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Features to DROP (not useful for modeling)\n",
    "drop_features = [\n",
    "    'Customer_ID',           # Unique identifier\n",
    "    'Customer_Name',         # Personal info\n",
    "    'Transaction_ID',        # Unique identifier\n",
    "    'Merchant_ID',           # Too many unique values\n",
    "    'Transaction_Date',      # Already extracted features\n",
    "    'Transaction_Time',      # Already extracted features\n",
    "    'Transaction_DateTime',  # Already extracted features\n",
    "    'Transaction_Location',  # Too many unique values (can use State/City instead)\n",
    "    'Customer_Contact',      # Personal info\n",
    "    'Customer_Email',        # Personal info\n",
    "    'Transaction_Currency',  # All same (INR)\n",
    "    'Bank_Branch',          # High cardinality, use State/City instead\n",
    "]\n",
    "\n",
    "# Categorical features (need encoding)\n",
    "categorical_features = [\n",
    "    'Gender',\n",
    "    'State',\n",
    "    'City', \n",
    "    'Account_Type',\n",
    "    'Transaction_Type',\n",
    "    'Merchant_Category',\n",
    "    'Transaction_Device',\n",
    "    'Device_Type',\n",
    "    'Transaction_Description',\n",
    "    'Amount_Category'\n",
    "]\n",
    "\n",
    "# Numerical features\n",
    "numerical_features = [\n",
    "    'Age',\n",
    "    'Transaction_Amount',\n",
    "    'Account_Balance',\n",
    "    'Hour',\n",
    "    'Day_of_Week',\n",
    "    'Day',\n",
    "    'Month',\n",
    "    'Transaction_to_Balance_Ratio',\n",
    "    'Is_Weekend',\n",
    "    'Is_Night',\n",
    "    'Is_Business_Hours',\n",
    "    'Is_High_Value',\n",
    "    'Is_Low_Balance'\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target = 'Is_Fraud'\n",
    "\n",
    "print(f\"Features to DROP: {len(drop_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"\\nTotal features for modeling: {len(categorical_features) + len(numerical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af37d957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HANDLING HIGH CARDINALITY FEATURES\n",
      "================================================================================\n",
      "Unique values in categorical features:\n",
      "  ‚Ä¢ Gender: 2 unique values\n",
      "  ‚Ä¢ State: 34 unique values\n",
      "  ‚Ä¢ City: 145 unique values\n",
      "  ‚Ä¢ Account_Type: 3 unique values\n",
      "  ‚Ä¢ Transaction_Type: 5 unique values\n",
      "  ‚Ä¢ Merchant_Category: 6 unique values\n",
      "  ‚Ä¢ Transaction_Device: 20 unique values\n",
      "  ‚Ä¢ Device_Type: 4 unique values\n",
      "  ‚Ä¢ Transaction_Description: 172 unique values\n",
      "  ‚Ä¢ Amount_Category: 4 unique values\n",
      "\n",
      "High cardinality features identified: ['State', 'City', 'Transaction_Description']\n",
      "These will be frequency-encoded AFTER train/test split to prevent data leakage\n",
      "\n",
      "‚úì Categorical features for one-hot encoding: 7\n",
      "‚úì Features to frequency-encode later: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"HANDLING HIGH CARDINALITY FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check cardinality of categorical features\n",
    "print(\"Unique values in categorical features:\")\n",
    "for col in categorical_features:\n",
    "    print(f\"  ‚Ä¢ {col}: {df[col].nunique()} unique values\")\n",
    "\n",
    "# Identify high cardinality features (we'll encode these AFTER train/test split)\n",
    "high_cardinality_features = ['State', 'City', 'Transaction_Description']\n",
    "\n",
    "print(f\"\\nHigh cardinality features identified: {high_cardinality_features}\")\n",
    "print(\"These will be frequency-encoded AFTER train/test split to prevent data leakage\")\n",
    "\n",
    "# Remove original high cardinality features from categorical list\n",
    "# (They'll be encoded later)\n",
    "categorical_features = [f for f in categorical_features if f not in high_cardinality_features]\n",
    "\n",
    "print(f\"\\n‚úì Categorical features for one-hot encoding: {len(categorical_features)}\")\n",
    "print(f\"‚úì Features to frequency-encode later: {len(high_cardinality_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e56062c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENCODING CATEGORICAL VARIABLES\n",
      "================================================================================\n",
      "\n",
      "Categorical features to encode: ['Gender', 'Account_Type', 'Transaction_Type', 'Merchant_Category', 'Transaction_Device', 'Device_Type', 'Amount_Category']\n",
      "Number of features: 7\n",
      "\n",
      "‚úì One-Hot Encoding applied\n",
      "Shape before encoding: (200000, 35)\n",
      "Shape after encoding: (200000, 65)\n",
      "New features created: 30\n",
      "\n",
      "‚úì High cardinality features preserved for later encoding:\n",
      "  ‚Ä¢ State: Present\n",
      "  ‚Ä¢ City: Present\n",
      "  ‚Ä¢ Transaction_Description: Present\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ENCODING CATEGORICAL VARIABLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# One-Hot Encoding for low cardinality categorical features ONLY\n",
    "print(f\"\\nCategorical features to encode: {categorical_features}\")\n",
    "print(f\"Number of features: {len(categorical_features)}\")\n",
    "\n",
    "# Apply One-Hot Encoding (State, City, Transaction_Description will stay as-is for now)\n",
    "df_encoded = pd.get_dummies(df_processed, columns=categorical_features, drop_first=True, dtype=int)\n",
    "\n",
    "print(f\"\\n‚úì One-Hot Encoding applied\")\n",
    "print(f\"Shape before encoding: {df_processed.shape}\")\n",
    "print(f\"Shape after encoding: {df_encoded.shape}\")\n",
    "print(f\"New features created: {df_encoded.shape[1] - df_processed.shape[1]}\")\n",
    "\n",
    "# Verify high cardinality features are still present\n",
    "print(f\"\\n‚úì High cardinality features preserved for later encoding:\")\n",
    "for col in high_cardinality_features:\n",
    "    if col in df_encoded.columns:\n",
    "        print(f\"  ‚Ä¢ {col}: Present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d76175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPARING FEATURES AND TARGET\n",
      "================================================================================\n",
      "Features (X) shape: (200000, 52)\n",
      "Target (y) shape: (200000,)\n",
      "\n",
      "Target distribution:\n",
      "Is_Fraud\n",
      "0    189912\n",
      "1     10088\n",
      "Name: count, dtype: int64\n",
      "Fraud percentage: 5.04%\n",
      "\n",
      "‚úì High cardinality features still in X (will be encoded after split):\n",
      "  ‚Ä¢ State: 34 unique values\n",
      "  ‚Ä¢ City: 145 unique values\n",
      "  ‚Ä¢ Transaction_Description: 172 unique values\n",
      "\n",
      "Total features: 52\n",
      "Next step: Train-test split, then frequency encoding\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PREPARING FEATURES AND TARGET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_model = df_encoded.drop(columns=drop_features, errors='ignore')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "# NOTE: State, City, Transaction_Description are still in X at this point\n",
    "X = df_model.drop(columns=[target])\n",
    "y = df_model[target]\n",
    "\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Fraud percentage: {y.mean()*100:.2f}%\")\n",
    "\n",
    "# Verify high cardinality features are present\n",
    "print(f\"\\n‚úì High cardinality features still in X (will be encoded after split):\")\n",
    "for col in high_cardinality_features:\n",
    "    if col in X.columns:\n",
    "        print(f\"  ‚Ä¢ {col}: {X[col].nunique()} unique values\")\n",
    "\n",
    "print(f\"\\nTotal features: {X.shape[1]}\")\n",
    "print(\"Next step: Train-test split, then frequency encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6a6154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAIN-TEST SPLIT (Hold-out Test Set)\n",
      "================================================================================\n",
      "Total samples: 200,000\n",
      "\n",
      "Data for K-Fold CV: 160,000 samples (80%)\n",
      "Hold-out test set: 40,000 samples (20%)\n",
      "\n",
      "==================================================\n",
      "K-Fold CV Data (Training Portion):\n",
      "==================================================\n",
      "Total samples: 160,000\n",
      "Is_Fraud\n",
      "0    151930\n",
      "1      8070\n",
      "Name: count, dtype: int64\n",
      "Fraud rate: 5.04%\n",
      "\n",
      "==================================================\n",
      "Hold-out Test Set (Final Evaluation):\n",
      "==================================================\n",
      "Total samples: 40,000\n",
      "Is_Fraud\n",
      "0    37982\n",
      "1     2018\n",
      "Name: count, dtype: int64\n",
      "Fraud rate: 5.04%\n",
      "\n",
      "==================================================\n",
      "Stratification Verification:\n",
      "==================================================\n",
      "Original fraud rate:    5.04%\n",
      "K-Fold data fraud rate: 5.04%\n",
      "Test set fraud rate:    5.04%\n",
      "\n",
      "‚úì Stratification successful - fraud rates match!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAIN-TEST SPLIT (Hold-out Test Set)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split: 80% for K-Fold CV, 20% held-out for final testing\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {len(X):,}\")\n",
    "print(f\"\\nData for K-Fold CV: {X_train_full.shape[0]:,} samples ({X_train_full.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"Hold-out test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"K-Fold CV Data (Training Portion):\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total samples: {len(y_train_full):,}\")\n",
    "print(y_train_full.value_counts())\n",
    "print(f\"Fraud rate: {y_train_full.mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Hold-out Test Set (Final Evaluation):\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total samples: {len(y_test):,}\")\n",
    "print(y_test.value_counts())\n",
    "print(f\"Fraud rate: {y_test.mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Stratification Verification:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Original fraud rate:    {y.mean()*100:.2f}%\")\n",
    "print(f\"K-Fold data fraud rate: {y_train_full.mean()*100:.2f}%\")\n",
    "print(f\"Test set fraud rate:    {y_test.mean()*100:.2f}%\")\n",
    "print(\"\\n‚úì Stratification successful - fraud rates match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f49648a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FREQUENCY ENCODING (NO DATA LEAKAGE)\n",
      "================================================================================\n",
      "\n",
      "Computing frequency maps on TRAINING data only...\n",
      "\n",
      "State:\n",
      "  ‚Ä¢ Unique values in training: 34\n",
      "  ‚Ä¢ Frequency range: 0.028600 to 0.030081\n",
      "  ‚Ä¢ Unseen values in test: 0 (filled with 0)\n",
      "\n",
      "City:\n",
      "  ‚Ä¢ Unique values in training: 145\n",
      "  ‚Ä¢ Frequency range: 0.005450 to 0.040725\n",
      "  ‚Ä¢ Unseen values in test: 0 (filled with 0)\n",
      "\n",
      "Transaction_Description:\n",
      "  ‚Ä¢ Unique values in training: 172\n",
      "  ‚Ä¢ Frequency range: 0.005256 to 0.006506\n",
      "  ‚Ä¢ Unseen values in test: 0 (filled with 0)\n",
      "\n",
      "================================================================================\n",
      "Removing original high cardinality columns...\n",
      "\n",
      "‚úì Frequency encoding complete (NO data leakage)\n",
      "Training set shape: (160000, 52)\n",
      "Test set shape: (40000, 52)\n",
      "\n",
      "Final feature count: 52\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FREQUENCY ENCODING (NO DATA LEAKAGE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define function for frequency encoding\n",
    "def compute_frequency_map(series):\n",
    "    \"\"\"Compute frequency map from a pandas Series\"\"\"\n",
    "    return series.value_counts(normalize=True).to_dict()\n",
    "\n",
    "# Apply frequency encoding to high cardinality features\n",
    "# CRITICAL: Compute frequencies ONLY on training data, then apply to test\n",
    "print(\"\\nComputing frequency maps on TRAINING data only...\")\n",
    "\n",
    "frequency_maps = {}\n",
    "for col in high_cardinality_features:\n",
    "    # Compute frequency on TRAINING data only\n",
    "    frequency_maps[col] = compute_frequency_map(X_train_full[col])\n",
    "    \n",
    "    # Apply to both train and test\n",
    "    X_train_full[f'{col}_Frequency'] = X_train_full[col].map(frequency_maps[col]).fillna(0)\n",
    "    X_test[f'{col}_Frequency'] = X_test[col].map(frequency_maps[col]).fillna(0)\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  ‚Ä¢ Unique values in training: {X_train_full[col].nunique()}\")\n",
    "    print(f\"  ‚Ä¢ Frequency range: {X_train_full[f'{col}_Frequency'].min():.6f} to {X_train_full[f'{col}_Frequency'].max():.6f}\")\n",
    "    print(f\"  ‚Ä¢ Unseen values in test: {X_test[col].map(frequency_maps[col]).isna().sum()} (filled with 0)\")\n",
    "\n",
    "# Now drop the original categorical columns\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Removing original high cardinality columns...\")\n",
    "X_train_full = X_train_full.drop(columns=high_cardinality_features)\n",
    "X_test = X_test.drop(columns=high_cardinality_features)\n",
    "\n",
    "print(f\"\\n‚úì Frequency encoding complete (NO data leakage)\")\n",
    "print(f\"Training set shape: {X_train_full.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "# Update feature names list\n",
    "feature_names = X_train_full.columns.tolist()\n",
    "print(f\"\\nFinal feature count: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7abc538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING QUANTILE-BASED FEATURES (NO DATA LEAKAGE)\n",
      "================================================================================\n",
      "Quantiles computed on training data only:\n",
      "  ‚Ä¢ 75th percentile Transaction Amount: ‚Çπ74,379.05\n",
      "  ‚Ä¢ 25th percentile Account Balance: ‚Çπ28,726.97\n",
      "\n",
      "‚úì Features created using training-set thresholds\n",
      "\n",
      "Training set:\n",
      "  High value transactions: 40,000 (25.0%)\n",
      "  Low balance accounts: 40,000 (25.0%)\n",
      "\n",
      "Test set:\n",
      "  High value transactions: 9,875 (24.7%)\n",
      "  Low balance accounts: 9,968 (24.9%)\n",
      "\n",
      "Final feature count: 54\n",
      "Training shape: (160000, 54)\n",
      "Test shape: (40000, 54)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CREATING QUANTILE-BASED FEATURES (NO DATA LEAKAGE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compute quantiles on TRAINING data only\n",
    "transaction_75th = X_train_full['Transaction_Amount'].quantile(0.75)\n",
    "balance_25th = X_train_full['Account_Balance'].quantile(0.25)\n",
    "\n",
    "print(f\"Quantiles computed on training data only:\")\n",
    "print(f\"  ‚Ä¢ 75th percentile Transaction Amount: ‚Çπ{transaction_75th:,.2f}\")\n",
    "print(f\"  ‚Ä¢ 25th percentile Account Balance: ‚Çπ{balance_25th:,.2f}\")\n",
    "\n",
    "# Apply to both train and test using the training-derived thresholds\n",
    "X_train_full['Is_High_Value'] = (X_train_full['Transaction_Amount'] > transaction_75th).astype(int)\n",
    "X_train_full['Is_Low_Balance'] = (X_train_full['Account_Balance'] < balance_25th).astype(int)\n",
    "\n",
    "X_test['Is_High_Value'] = (X_test['Transaction_Amount'] > transaction_75th).astype(int)\n",
    "X_test['Is_Low_Balance'] = (X_test['Account_Balance'] < balance_25th).astype(int)\n",
    "\n",
    "print(f\"\\n‚úì Features created using training-set thresholds\")\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  High value transactions: {X_train_full['Is_High_Value'].sum():,} ({X_train_full['Is_High_Value'].mean()*100:.1f}%)\")\n",
    "print(f\"  Low balance accounts: {X_train_full['Is_Low_Balance'].sum():,} ({X_train_full['Is_Low_Balance'].mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  High value transactions: {X_test['Is_High_Value'].sum():,} ({X_test['Is_High_Value'].mean()*100:.1f}%)\")\n",
    "print(f\"  Low balance accounts: {X_test['Is_Low_Balance'].sum():,} ({X_test['Is_Low_Balance'].mean()*100:.1f}%)\")\n",
    "\n",
    "# Update feature count\n",
    "feature_names = X_train_full.columns.tolist()\n",
    "print(f\"\\nFinal feature count: {len(feature_names)}\")\n",
    "print(f\"Training shape: {X_train_full.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aff7c8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRATIFIED K-FOLD CROSS-VALIDATION STRATEGY\n",
      "================================================================================\n",
      "Strategy: 5-Fold Stratified Cross-Validation\n",
      "\n",
      "How it works:\n",
      "  1. Split 160,000 samples into 5 folds\n",
      "  2. Each fold maintains ~5.04% fraud rate\n",
      "  3. For each fold:\n",
      "     ‚Ä¢ Train on 4 folds (~128000 samples)\n",
      "     ‚Ä¢ Validate on 1 fold (~32000 samples)\n",
      "     ‚Ä¢ Apply SMOTE to training folds only\n",
      "     ‚Ä¢ Scale features on training folds, apply to validation\n",
      "     ‚Ä¢ Train model and evaluate on validation fold\n",
      "  4. Average performance across all 5 folds\n",
      "  5. Final model trained on all 160,000 samples\n",
      "  6. Final evaluation on hold-out test set (40,000 samples)\n",
      "\n",
      "================================================================================\n",
      "FOLD BREAKDOWN - Verification of Stratification:\n",
      "================================================================================\n",
      "\n",
      "Fold 1:\n",
      "  Training:   128,000 samples | Fraud: 6,456 ( 5.04%)\n",
      "  Validation:  32,000 samples | Fraud: 1,614 ( 5.04%)\n",
      "\n",
      "Fold 2:\n",
      "  Training:   128,000 samples | Fraud: 6,456 ( 5.04%)\n",
      "  Validation:  32,000 samples | Fraud: 1,614 ( 5.04%)\n",
      "\n",
      "Fold 3:\n",
      "  Training:   128,000 samples | Fraud: 6,456 ( 5.04%)\n",
      "  Validation:  32,000 samples | Fraud: 1,614 ( 5.04%)\n",
      "\n",
      "Fold 4:\n",
      "  Training:   128,000 samples | Fraud: 6,456 ( 5.04%)\n",
      "  Validation:  32,000 samples | Fraud: 1,614 ( 5.04%)\n",
      "\n",
      "Fold 5:\n",
      "  Training:   128,000 samples | Fraud: 6,456 ( 5.04%)\n",
      "  Validation:  32,000 samples | Fraud: 1,614 ( 5.04%)\n",
      "\n",
      "================================================================================\n",
      "‚úì All folds maintain consistent ~5.04% fraud rate!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STRATIFIED K-FOLD CROSS-VALIDATION STRATEGY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Initialize Stratified K-Fold\n",
    "n_splits = 5  # We'll use 5 folds\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Strategy: {n_splits}-Fold Stratified Cross-Validation\")\n",
    "print(f\"\\nHow it works:\")\n",
    "print(f\"  1. Split {X_train_full.shape[0]:,} samples into {n_splits} folds\")\n",
    "print(f\"  2. Each fold maintains ~{y_train_full.mean()*100:.2f}% fraud rate\")\n",
    "print(f\"  3. For each fold:\")\n",
    "print(f\"     ‚Ä¢ Train on {n_splits-1} folds (~{X_train_full.shape[0]*(n_splits-1)/n_splits:.0f} samples)\")\n",
    "print(f\"     ‚Ä¢ Validate on 1 fold (~{X_train_full.shape[0]/n_splits:.0f} samples)\")\n",
    "print(f\"     ‚Ä¢ Apply SMOTE to training folds only\")\n",
    "print(f\"     ‚Ä¢ Scale features on training folds, apply to validation\")\n",
    "print(f\"     ‚Ä¢ Train model and evaluate on validation fold\")\n",
    "print(f\"  4. Average performance across all {n_splits} folds\")\n",
    "print(f\"  5. Final model trained on all {X_train_full.shape[0]:,} samples\")\n",
    "print(f\"  6. Final evaluation on hold-out test set ({X_test.shape[0]:,} samples)\")\n",
    "\n",
    "# Demonstrate the splits\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FOLD BREAKDOWN - Verification of Stratification:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train_full, y_train_full), 1):\n",
    "    y_train_fold = y_train_full.iloc[train_idx]\n",
    "    y_val_fold = y_train_full.iloc[val_idx]\n",
    "    \n",
    "    fraud_train = y_train_fold.sum()\n",
    "    fraud_val = y_val_fold.sum()\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx}:\")\n",
    "    print(f\"  Training:   {len(train_idx):>7,} samples | Fraud: {fraud_train:>5,} ({y_train_fold.mean()*100:>5.2f}%)\")\n",
    "    print(f\"  Validation: {len(val_idx):>7,} samples | Fraud: {fraud_val:>5,} ({y_val_fold.mean()*100:>5.2f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úì All folds maintain consistent ~5.04% fraud rate!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0487276b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAVING PREPROCESSED DATA\n",
      "================================================================================\n",
      "Saving files...\n",
      "\n",
      "‚úì All files saved successfully!\n",
      "\n",
      "Saved files in '../data/preprocessed/':\n",
      "  1. X_train_full.npy         - Training data for K-Fold CV (160,000 samples)\n",
      "  2. y_train_full.npy         - Training labels\n",
      "  3. X_test.npy               - Hold-out test set (40,000 samples)\n",
      "  4. X_test_scaled.npy        - Scaled hold-out test set\n",
      "  5. y_test.npy               - Test labels\n",
      "  6. scaler.pkl               - Fitted StandardScaler\n",
      "  7. feature_names.txt        - List of all 54 features\n",
      "  8. preprocessing_params.pkl - All preprocessing parameters + thresholds + frequency maps\n",
      "\n",
      "‚úì Preprocessing complete with NO DATA LEAKAGE!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SAVING PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create directory for preprocessed data\n",
    "os.makedirs('../data/preprocessed', exist_ok=True)\n",
    "\n",
    "# Fit scaler on full training data (for deployment)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_full)\n",
    "\n",
    "# Scale the hold-out test set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the data\n",
    "print(\"Saving files...\")\n",
    "\n",
    "# Training data for K-Fold CV (unscaled - we'll scale inside each fold)\n",
    "np.save('../data/preprocessed/X_train_full.npy', X_train_full.values)\n",
    "np.save('../data/preprocessed/y_train_full.npy', y_train_full.values)\n",
    "\n",
    "# Hold-out test set\n",
    "np.save('../data/preprocessed/X_test.npy', X_test.values)\n",
    "np.save('../data/preprocessed/X_test_scaled.npy', X_test_scaled)\n",
    "np.save('../data/preprocessed/y_test.npy', y_test.values)\n",
    "\n",
    "# Save the scaler for deployment\n",
    "joblib.dump(scaler, '../data/preprocessed/scaler.pkl')\n",
    "\n",
    "# Save feature names\n",
    "with open('../data/preprocessed/feature_names.txt', 'w') as f:\n",
    "    for col in feature_names:\n",
    "        f.write(f\"{col}\\n\")\n",
    "\n",
    "# Save preprocessing parameters (including thresholds for deployment)\n",
    "params = {\n",
    "    'n_folds': 5,\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'smote_strategy': 0.5,\n",
    "    'total_features': len(feature_names),\n",
    "    'feature_names': feature_names,\n",
    "    'transaction_75th': transaction_75th,  # Save for deployment\n",
    "    'balance_25th': balance_25th,          # Save for deployment\n",
    "    'frequency_maps': frequency_maps        # Save for deployment\n",
    "}\n",
    "joblib.dump(params, '../data/preprocessed/preprocessing_params.pkl')\n",
    "\n",
    "print(\"\\n‚úì All files saved successfully!\")\n",
    "print(\"\\nSaved files in '../data/preprocessed/':\")\n",
    "print(\"  1. X_train_full.npy         - Training data for K-Fold CV (160,000 samples)\")\n",
    "print(\"  2. y_train_full.npy         - Training labels\")\n",
    "print(\"  3. X_test.npy               - Hold-out test set (40,000 samples)\")\n",
    "print(\"  4. X_test_scaled.npy        - Scaled hold-out test set\")\n",
    "print(\"  5. y_test.npy               - Test labels\")\n",
    "print(\"  6. scaler.pkl               - Fitted StandardScaler\")\n",
    "print(\"  7. feature_names.txt        - List of all 54 features\")\n",
    "print(\"  8. preprocessing_params.pkl - All preprocessing parameters + thresholds + frequency maps\")\n",
    "\n",
    "print(f\"\\n‚úì Preprocessing complete with NO DATA LEAKAGE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0657beb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPROCESSING COMPLETE - FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìä ORIGINAL DATA:\n",
      "  ‚Ä¢ Total samples: 200,000\n",
      "  ‚Ä¢ Original features: 35\n",
      "  ‚Ä¢ Fraud cases: 10,088 (5.04%)\n",
      "\n",
      "üîß FEATURE ENGINEERING:\n",
      "  ‚Ä¢ Time-based features created: 6\n",
      "    (Hour, Day, Month, Is_Weekend, Is_Night, Is_Business_Hours)\n",
      "  ‚Ä¢ Transaction features created: 4\n",
      "    (Transaction_to_Balance_Ratio, Is_High_Value, Is_Low_Balance, Amount_Category)\n",
      "  ‚Ä¢ Frequency encoding applied: 3\n",
      "    (State_Frequency, City_Frequency, Transaction_Description_Frequency)\n",
      "\n",
      "üìù ENCODING:\n",
      "  ‚Ä¢ Categorical features one-hot encoded: 7\n",
      "  ‚Ä¢ Total features after encoding: 52\n",
      "\n",
      "‚úÇÔ∏è DATA SPLIT:\n",
      "  ‚Ä¢ K-Fold CV data (80%): 160,000 samples\n",
      "    - Fraud: 8,070 (5.04%)\n",
      "  ‚Ä¢ Hold-out test set (20%): 40,000 samples\n",
      "    - Fraud: 2,018 (5.04%)\n",
      "\n",
      "üîÑ CROSS-VALIDATION STRATEGY:\n",
      "  ‚Ä¢ Method: 5-Fold Stratified Cross-Validation\n",
      "  ‚Ä¢ Training per fold: 128,000 samples\n",
      "  ‚Ä¢ Validation per fold: 32,000 samples\n",
      "  ‚Ä¢ SMOTE strategy: 0.5 (increase fraud to 50% of non-fraud)\n",
      "  ‚Ä¢ Scaling: StandardScaler (applied inside each fold)\n",
      "\n",
      "üíæ SAVED FILES:\n",
      "  ‚Ä¢ Location: ../data/preprocessed/\n",
      "  ‚Ä¢ Total size: ~100 MB\n",
      "  ‚Ä¢ Files: 8 (data arrays, scaler, parameters, feature names)\n",
      "\n",
      "‚úÖ PREPROCESSING COMPLETE!\n",
      "\n",
      "================================================================================\n",
      "READY FOR MODEL TRAINING!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PREPROCESSING COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä ORIGINAL DATA:\")\n",
    "print(f\"  ‚Ä¢ Total samples: {len(df):,}\")\n",
    "print(f\"  ‚Ä¢ Original features: {df.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Fraud cases: {df['Is_Fraud'].sum():,} ({df['Is_Fraud'].mean()*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nüîß FEATURE ENGINEERING:\")\n",
    "print(f\"  ‚Ä¢ Time-based features created: 6\")\n",
    "print(f\"    (Hour, Day, Month, Is_Weekend, Is_Night, Is_Business_Hours)\")\n",
    "print(f\"  ‚Ä¢ Transaction features created: 4\")\n",
    "print(f\"    (Transaction_to_Balance_Ratio, Is_High_Value, Is_Low_Balance, Amount_Category)\")\n",
    "print(f\"  ‚Ä¢ Frequency encoding applied: 3\")\n",
    "print(f\"    (State_Frequency, City_Frequency, Transaction_Description_Frequency)\")\n",
    "\n",
    "print(\"\\nüìù ENCODING:\")\n",
    "print(f\"  ‚Ä¢ Categorical features one-hot encoded: 7\")\n",
    "print(f\"  ‚Ä¢ Total features after encoding: {X.shape[1]}\")\n",
    "\n",
    "print(\"\\n‚úÇÔ∏è DATA SPLIT:\")\n",
    "print(f\"  ‚Ä¢ K-Fold CV data (80%): {len(X_train_full):,} samples\")\n",
    "print(f\"    - Fraud: {y_train_full.sum():,} ({y_train_full.mean()*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Hold-out test set (20%): {len(X_test):,} samples\")\n",
    "print(f\"    - Fraud: {y_test.sum():,} ({y_test.mean()*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nüîÑ CROSS-VALIDATION STRATEGY:\")\n",
    "print(f\"  ‚Ä¢ Method: {n_splits}-Fold Stratified Cross-Validation\")\n",
    "print(f\"  ‚Ä¢ Training per fold: {X_train_full.shape[0]*(n_splits-1)//n_splits:,} samples\")\n",
    "print(f\"  ‚Ä¢ Validation per fold: {X_train_full.shape[0]//n_splits:,} samples\")\n",
    "print(f\"  ‚Ä¢ SMOTE strategy: 0.5 (increase fraud to 50% of non-fraud)\")\n",
    "print(f\"  ‚Ä¢ Scaling: StandardScaler (applied inside each fold)\")\n",
    "\n",
    "print(\"\\nüíæ SAVED FILES:\")\n",
    "print(f\"  ‚Ä¢ Location: ../data/preprocessed/\")\n",
    "print(f\"  ‚Ä¢ Total size: ~100 MB\")\n",
    "print(f\"  ‚Ä¢ Files: 8 (data arrays, scaler, parameters, feature names)\")\n",
    "\n",
    "print(\"\\n‚úÖ PREPROCESSING COMPLETE!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"READY FOR MODEL TRAINING!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
